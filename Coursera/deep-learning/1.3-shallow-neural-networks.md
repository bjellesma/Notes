<img width="1306" height="668" alt="image" src="https://github.com/user-attachments/assets/9569f82f-002d-46dc-9b4f-5693af5c8ca0" />

The above screenshot is a 2 layer neural network where the first layer is the hidden layer and the 2nd layer is the output layer. The input layer is referred to as layer 0.

Because we're vectorizing and using the dot product, we want to use $z=w^Tx+b$ where T means that we're transposing. Previously, we've used just $z=wx+b$ but this is because we only dealt with one feature at a time. Because we're now dealing with multiple features, w and x are both vectors and we need to compute the dot product. In linear algebra, we need the dimensions to align in order to get the dot product meaning we have to transpose w from being a column vector to a row vector.

Technically with 1D vectors, we can just multiply element-wise to get the dot product. Transposing doesn't change the result. However, when we represent vectors as 2D matrices (column vectors) or batch multiple examples into a matrix, the transpose is required for the dimensions to align.

Remember from linear algebra that when doing matrix multiplication for the dot product, the inner dimension of the first matrix must match the inner dimension of the second matrix. For (m×n) · (n×p) = (m×p), the two n's must match. 

Keep in mind that sometimes numpy will be able to broadcast like making a 3x1 matrix into a 3x3 to match the dimensions of the second matrix.

In this screenshot, we're using the sigmoid as an activation function on every layer. Sigmoid outputs a value between 0 and 1, which is especially useful on the output layer for binary classification since it can be interpreted as a probability.

<img width="588" height="508" alt="image" src="https://github.com/user-attachments/assets/fc39b40a-7592-4724-b2c3-b37c054d34b0" />

In the below screenshot notice how the second layer uses the activation function of the previous layer as input.

<img width="1182" height="288" alt="image" src="https://github.com/user-attachments/assets/8942c8f4-1366-4c30-b098-71968f939815" />

It's actually standard practice that different activation functions are used on different layers.



A typical modern network might look like:

Hidden layers: ReLU (or variants like Leaky ReLU, GELU)
Output layer: Depends on the task

Sigmoid for binary classification
Softmax for multi-class classification
Linear (no activation) for regression

## Activation functions

Besides the sigmoid function being used in the output layer for binary classification, it's more common to see the **tanh** activation function used in a hidden layer

$$a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

The tanh function is bound from -1 to 1 on the y axis

<img width="369" height="160" alt="image" src="https://github.com/user-attachments/assets/e3dcc6f4-5697-4c27-bcaf-77d6c9325769" />

Another very common activation function to see in the hidden layer is **RELU** which is $a = max(0,z)$ where z is a positive function 

<img width="473" height="233" alt="image" src="https://github.com/user-attachments/assets/23427109-f4c8-463d-95f8-405ea1168ae6" />

Sometimes people will also use **leaky RELU** where there is a slight bend 

<img width="571" height="309" alt="image" src="https://github.com/user-attachments/assets/ef6fbad2-3da5-4834-aa24-f9e0a9e2d3b1" />

It should be noted that we want non linear activation functions for at least some layers. This is because if we continuously multiply linear functions like wx+b, we get very uninteresting results that we could've gotten with a single matrix multiplication. By using a non linear activation like tanh or RELU, we produce more interesting results. By interesting, the network can learn complex, non-linear relationships.

## Gradient Descent for Neural Networks

For the sigmoid, $g(z) = \frac{1}{1 + e^{-z}}$, $\frac{d}{dz}g(z) = g(z)(1 - g(z))$

This makes intuitive sense because

$$
z = 10, \quad g(z) \approx 1

\frac{d}{dz}g(z) \approx 1(1-1) \approx 0
$$

$$
z = -10, \quad g(z) \approx 0

\frac{d}{dz}g(z) \approx 0(1-0) \approx 0
$$

$$
z = 0, \quad g(z) = \frac{1}{2}

\frac{d}{dz}g(z) = \frac{1}{2}\left(1 - \frac{1}{2}\right) = \frac{1}{4}
$$

This nicely illustrates the vanishing gradient problem with sigmoid—at the extremes (z = 10 or z = -10), the derivative is essentially zero, so learning stalls. The gradient is only meaningful near z = 0, which is part of why ReLU became more popular for deep networks.
