<img width="1306" height="668" alt="image" src="https://github.com/user-attachments/assets/9569f82f-002d-46dc-9b4f-5693af5c8ca0" />

The above screenshot is a 2 layer neural network where the first layer is the hidden layer and the 2nd layer is the output layer. The input layer is referred to as layer 0.

Because we're vectorizing and using the dot product, we want to use $z=w^Tx+b$ where T means that we're transposing. Previously, we've used just $z=wx+b$ but this is because we only dealt with one feature at a time. Because we're now dealing with multiple features, w and x are both vectors and we need to compute the dot product. In linear algebra, we need the dimensions to align in order to get the dot product meaning we have to transpose w from being a column vector to a row vector.

Technically with 1D vectors, we can just multiply element-wise to get the dot product. Transposing doesn't change the result. However, when we represent vectors as 2D matrices (column vectors) or batch multiple examples into a matrix, the transpose is required for the dimensions to align.

Remember from linear algebra that when doing matrix multiplication for the dot product, the inner dimension of the first matrix must match the inner dimension of the second matrix. For (m×n) · (n×p) = (m×p), the two n's must match. 

Keep in mind that sometimes numpy will be able to broadcast like making a 3x1 matrix into a 3x3 to match the dimensions of the second matrix.

In this screenshot, we're using the sigmoid as an activation function on every layer. Sigmoid outputs a value between 0 and 1, which is especially useful on the output layer for binary classification since it can be interpreted as a probability.

<img width="588" height="508" alt="image" src="https://github.com/user-attachments/assets/fc39b40a-7592-4724-b2c3-b37c054d34b0" />

In the below screenshot notice how the second layer uses the activation function of the previous layer as input.

<img width="1182" height="288" alt="image" src="https://github.com/user-attachments/assets/8942c8f4-1366-4c30-b098-71968f939815" />

It's actually standard practice that different activation functions are used on different layers.



A typical modern network might look like:

Hidden layers: ReLU (or variants like Leaky ReLU, GELU)
Output layer: Depends on the task

Sigmoid for binary classification
Softmax for multi-class classification
Linear (no activation) for regression

## Activation functions

Besides the sigmoid function being used in the output layer for binary classification, it's more common to see the **tanh** activation function used in a hidden layer

$$a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

The tanh function is bound from -1 to 1 on the y axis

<img width="369" height="160" alt="image" src="https://github.com/user-attachments/assets/e3dcc6f4-5697-4c27-bcaf-77d6c9325769" />

Another very common activation function to see in the hidden layer is **RELU** which is $a = max(0,z)$ where z is a positive function 

<img width="473" height="233" alt="image" src="https://github.com/user-attachments/assets/23427109-f4c8-463d-95f8-405ea1168ae6" />

Sometimes people will also use **leaky RELU** where there is a slight bend 

<img width="571" height="309" alt="image" src="https://github.com/user-attachments/assets/ef6fbad2-3da5-4834-aa24-f9e0a9e2d3b1" />

When activation functions are linear, they are usually referred to as **identity** because they yield uninteresting results. They are primarily used in output layers for regression problems. When you need to predict a continuous, unbounded value (like house prices, temperature, stock returns), the final layer uses a linear activation so the network can output any real number.

It should be noted that we want non linear activation functions for at least some layers. This is because if we continuously multiply linear functions like wx+b, we get very uninteresting results that we could've gotten with a single matrix multiplication. By using a non linear activation like tanh or RELU, we produce more interesting results. By interesting, the network can learn complex, non-linear relationships.

## Gradient Descent for Neural Networks

### Sigmoid
For the sigmoid, $g(z) = \frac{1}{1 + e^{-z}}$, $\frac{d}{dz}g(z) = g(z)(1 - g(z))$

This makes intuitive sense because

$$
z = 10, \quad g(z) \approx 1
\frac{d}{dz}g(z) \approx 1(1-1) \approx 0
$$

$$
z = -10, \quad g(z) \approx 0
\frac{d}{dz}g(z) \approx 0(1-0) \approx 0
$$

$$
z = 0, \quad g(z) = \frac{1}{2}
\frac{d}{dz}g(z) = \frac{1}{2}\left(1 - \frac{1}{2}\right) = \frac{1}{4}
$$

This nicely illustrates the vanishing gradient problem with sigmoid—at the extremes (z = 10 or z = -10), the derivative is essentially zero, so learning stalls. The gradient is only meaningful near z = 0, which is part of why ReLU became more popular for deep networks.

### tanh

For the tanh function $g(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$, $g'(z) = \frac{d}{dz}g(z) = 1 - (\tanh(z))^2$

$$z = 10, \quad \tanh(z) \approx 1, \quad g'(z) \approx 0$$

$$z = -10, \quad \tanh(z) \approx -1, \quad g'(z) \approx 0$$

$$z = 0, \quad \tanh(z) = 0, \quad g'(z) = 1$$

Notice tanh has the same vanishing gradient issue at the extremes, but it's centered around zero (output range -1 to 1) which often makes it preferable to sigmoid for hidden layers—the mean activation is closer to zero, which can help with learning dynamics.

### ReLU and Leaky ReLU

For ReLU $g(z) = \max(0, z)$, 

$$
g'(z) = \begin{cases} 
0 & \text{if } z < 0 
\\ 1 & \text{if } z > 0 
\\ \text{undefined} & \text{if } z = 0 \end{cases}
$$

This is what makes ReLU so popular: the gradient is either 0 or 1, never the tiny fractional values you get with sigmoid/tanh. No vanishing gradient on the positive side, and it's dead simple to compute. However, this does lead to the dying ReLU problem where neurons get suck at zero and never recover because their gradient is always zero. With Leaky ReLU, there's always some gradient flowing back, even for negative inputs.

Similarly, the derivative of leaky ReLU is just replacing 0 with an arbitrarily small number $$g(z) = \max(0.01z, z)$$

$$
g'(z) = \begin{cases} 0.01 & \text{if } z < 0 \\ 1 & \text{if } z > 0 \end{cases}
$$

### Formulas for gradient descent

<img width="2313" height="986" alt="image" src="https://github.com/user-attachments/assets/7abed20f-f748-4d25-9159-898f27d2f2aa" />

Here are the equations for if you're doingg a single training example at a time.

$$dz^{[2]} = a^{[2]} - y$$

$$dW^{[2]} = dz^{[2]} a^{[1]T}$$

$$db^{[2]} = dz^{[2]}$$

$$dz^{[1]} = W^{[2]T} dz^{[2]} * g^{[1]'}(z^{[1]})$$

$$dW^{[1]} = dz^{[1]} x^T$$

$$db^{[1]} = dz^{[1]}$$

The following are the equations for the vectorized form:

$$dZ^{[2]} = A^{[2]} - Y$$

$$dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T}$$

$$db^{[2]} = \frac{1}{m} np.sum(dZ^{[2]}, axis=1, keepdims=True)$$

$$dZ^{[1]} = W^{[2]T} dZ^{[2]} * g^{[1]'}(Z^{[1]})$$

$$dW^{[1]} = \frac{1}{m} dZ^{[1]} X^T$$

$$db^{[1]} = \frac{1}{m} np.sum(dZ^{[1]}, axis=1, keepdims=True)$$

The key differences from the single-example version: capital letters (matrices instead of vectors), the 1/m averaging factor, and summing across the m examples for the bias gradients

The following is python code

```python
def backward_propagation(parameters, cache, X, Y):
    """
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary "parameters".
    #(≈ 2 lines of code)
    # W1 = ...
    # W2 = ...
    # YOUR CODE STARTS HERE
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    
    dZ2 = A2 - Y
    dW2 = np.multiply(1/m,np.dot(dZ2,A1.T))
    db2 = np.multiply(1/m,np.sum(dZ2,axis=1,keepdims=True))
    dZ1 = np.multiply(np.dot(W2.T,dZ2),(1 - np.power(A1, 2)))
    dW1 = np.multiply(1/m,np.dot(dZ1, X.T))
    db1 = np.multiply(1/m,np.sum(dZ1,axis=1,keepdims=True))

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
```

## Random Initialization

In neural networks, you can initialize the bias matrix to zeros but you can't initialize the weights matrix to zeros because then the activation functions will all produce the same values. Instead, you need to randomly initialize the weights.

If all weights are initialized to zero (or any identical value), every neuron in a layer receives the same input, computes the same output, and crucially, receives the same gradient during backpropagation. So they all update identically and remain identical forever. You essentially have one neuron repeated many times, wasting all that capacity. This is called the **symetry problem**

<img width="2109" height="958" alt="image" src="https://github.com/user-attachments/assets/05813990-f493-4c61-ae7f-5e3e0a31d102" />


As you can see, you want to multiply the randomness by a very small number (0.01). This is because of the **vanishing gradients** problem where the learning rates will become very slow as the outer parts of the sigmoid function are reached. 

### keepdims

keepdims=True preserves the reduced dimension as a size-1 axis instead of collapsing it entirely.

```python
import numpy as np

A = np.array([[1, 2, 3],
              [4, 5, 6]])  # shape (2, 3)

# Without keepdims
A.sum(axis=1)          # array([6, 15])  — shape (2,)

# With keepdims
A.sum(axis=1, keepdims=True)  # array([[6],
                               #        [15]]) — shape (2, 1)
```

This is important because it enables broadcasting to work correctly. 

```python
# This works
A / A.sum(axis=1, keepdims=True)

# This fails or gives wrong results
A / A.sum(axis=1)  # shape mismatch (2,3) vs (2,)
```

In deep learning, you'll use it constantly for things like softmax, normalization, and computing means across batches or features while maintaining compatible shapes for subsequent operations.

### Programming Assignment Notes

<img width="1467" height="642" alt="image" src="https://github.com/user-attachments/assets/b86272a0-7f80-47a4-8cdc-0af2a888eb1b" />

In the above images, it's not linearly seperable data because we can't draw a straight line between the reds and the blues. This is why we want to use a neural network

```python
def compute_cost(A2, Y):
    """
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost given equation (13)
    
    """
    
    m = Y.shape[1] # number of examples
    logprobs = np.multiply(np.log(A2),Y) + np.multiply(1-Y, np.log(1-A2))
    cost = - np.multiply(1/m, np.sum(logprobs)) 
    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. 
                                    # E.g., turns [[17]] into 17. Removing redundant dimensions
    
    return cost
```

The above code models 

$$
J = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log(a^{[2](i)}) + (1 - y^{(i)}) \log(1 - a^{[2](i)}) \right)
$$

This can be used in a neural network like

```python
def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """    
    #(≈ 4 lines of code)
    # W1 = ...
    # b1 = ...
    # W2 = ...
    # b2 = ...
    # YOUR CODE STARTS HERE
    W1 = np.random.randn(n_h,n_x) * 0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h) * 0.01
    b2 = np.zeros((n_y,1))
    
    # YOUR CODE ENDS HERE

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """
    # Retrieve each parameter from the dictionary "parameters"
    #(≈ 4 lines of code)
    # W1 = ...
    # b1 = ...
    # W2 = ...
    # b2 = ...
    # YOUR CODE STARTS HERE
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # YOUR CODE ENDS HERE
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    # (≈ 4 lines of code)
    # Z1 = ...
    # A1 = ...
    # Z2 = ...
    # A2 = ...
    # YOUR CODE STARTS HERE
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    
    # YOUR CODE ENDS HERE
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache

def compute_cost(A2, Y):
    """
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost given equation (13)
    
    """
    
    m = Y.shape[1] # number of examples

    # Compute the cross-entropy cost
    # (≈ 2 lines of code)
    # logprobs = ...
    # cost = ...
    # YOUR CODE STARTS HERE
    logprobs = np.multiply(np.log(A2),Y) + np.multiply(1-Y, np.log(1-A2))
    cost = - np.multiply(1/m, np.sum(logprobs)) 
    
    # YOUR CODE ENDS HERE
    
    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. 
                                    # E.g., turns [[17]] into 17 
    
    return cost

def backward_propagation(parameters, cache, X, Y):
    """
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary "parameters".
    #(≈ 2 lines of code)
    # W1 = ...
    # W2 = ...
    # YOUR CODE STARTS HERE
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    
    # YOUR CODE ENDS HERE
        
    # Retrieve also A1 and A2 from dictionary "cache".
    #(≈ 2 lines of code)
    # A1 = ...
    # A2 = ...
    # YOUR CODE STARTS HERE
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    
    # YOUR CODE ENDS HERE
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    #(≈ 6 lines of code, corresponding to 6 equations on slide above)
    # dZ2 = ...
    # dW2 = ...
    # db2 = ...
    # dZ1 = ...
    # dW1 = ...
    # db1 = ...
    # YOUR CODE STARTS HERE
    dZ2 = A2 - Y
    dW2 = np.multiply(1/m,np.dot(dZ2,A1.T))
    db2 = np.multiply(1/m,np.sum(dZ2,axis=1,keepdims=True))
    dZ1 = np.multiply(np.dot(W2.T,dZ2),(1 - np.power(A1, 2)))
    dW1 = np.multiply(1/m,np.dot(dZ1, X.T))
    db1 = np.multiply(1/m,np.sum(dZ1,axis=1,keepdims=True))
    
    
    # YOUR CODE ENDS HERE
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads

def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """
    # Retrieve a copy of each parameter from the dictionary "parameters". Use copy.deepcopy(...) for W1 and W2
    #(≈ 4 lines of code)
    # W1 = ...
    # b1 = ...
    # W2 = ...
    # b2 = ...
    # YOUR CODE STARTS HERE
    W1 = copy.deepcopy(parameters["W1"])
    b1 = parameters["b1"]
    W2 = copy.deepcopy(parameters["W2"])
    b2 = parameters["b2"]
    
    # YOUR CODE ENDS HERE
    
    # Retrieve each gradient from the dictionary "grads"
    #(≈ 4 lines of code)
    # dW1 = ...
    # db1 = ...
    # dW2 = ...
    # db2 = ...
    # YOUR CODE STARTS HERE
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    
    # YOUR CODE ENDS HERE
    
    # Update rule for each parameter
    #(≈ 4 lines of code)
    # W1 = ...
    # b1 = ...
    # W2 = ...
    # b2 = ...
    # YOUR CODE STARTS HERE
    W1 = W1 - np.multiply(learning_rate, dW1)
    b1 = b1 - np.multiply(learning_rate, db1)
    W2 = W2 - np.multiply(learning_rate, dW2)
    b2 = b2 - np.multiply(learning_rate, db2)
    
    # YOUR CODE ENDS HERE
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters
    #(≈ 1 line of code)
    # parameters = ...
    # YOUR CODE STARTS HERE
    parameters = initialize_parameters(n_x, n_h, n_y)
    
    # YOUR CODE ENDS HERE
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        #(≈ 4 lines of code)
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
#         A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: "A2, Y". Outputs: "cost".
#         cost = compute_cost(A2, Y)
 
        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
#         grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
#         parameters = update_parameters(parameters, grads)
        
        # YOUR CODE STARTS HERE
        A2, cache = forward_propagation(X, parameters)
        cost = compute_cost(A2, Y)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads)
        
        # YOUR CODE ENDS HERE
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters

nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False)

```

We can just make a prediction with .5 as the threshold

```python
def predict(parameters, X):
    """
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
    #(≈ 2 lines of code)
    # A2, cache = ...
    # predictions = ...
    # YOUR CODE STARTS HERE
    A2, cache = forward_propagation(X, parameters)
    predictions = A2 > .5
```

The Accuracy of our dataset is now very high

<img width="1797" height="1242" alt="image" src="https://github.com/user-attachments/assets/3736056a-f0f7-470d-8806-d90137b00170" />
