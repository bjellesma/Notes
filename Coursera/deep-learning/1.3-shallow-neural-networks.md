<img width="1306" height="668" alt="image" src="https://github.com/user-attachments/assets/9569f82f-002d-46dc-9b4f-5693af5c8ca0" />

The above screenshot is a 2 layer neural network where the first layer is the hidden layer and the 2nd layer is the output layer. The input layer is referred to as layer 0.

Because we're vectorizing and using the dot product, we want to use $z=w^Tx+b$ where T means that we're transposing. Previously, we've used just $z=wx+b$ but this is because we only dealt with one feature at a time. Because we're now dealing with multiple features, w and x are both vectors and we need to compute the dot product. In linear algebra, we need the dimensions to align in order to get the dot product meaning we have to transpose w from being a column vector to a row vector.

Technically with 1D vectors, we can just multiply element-wise to get the dot product. Transposing doesn't change the result. However, when we represent vectors as 2D matrices (column vectors) or batch multiple examples into a matrix, the transpose is required for the dimensions to align.

Remember from linear algebra that when doing matrix multiplication for the dot product, the inner dimension of the first matrix must match the inner dimension of the second matrix. For (m×n) · (n×p) = (m×p), the two n's must match. 

Keep in mind that sometimes numpy will be able to broadcast like making a 3x1 matrix into a 3x3 to match the dimensions of the second matrix.

In this screenshot, we're using the sigmoid as an activation function on every layer. Sigmoid outputs a value between 0 and 1, which is especially useful on the output layer for binary classification since it can be interpreted as a probability.

<img width="588" height="508" alt="image" src="https://github.com/user-attachments/assets/fc39b40a-7592-4724-b2c3-b37c054d34b0" />

In the below screenshot notice how the second layer uses the activation function of the previous layer as input.

<img width="1182" height="288" alt="image" src="https://github.com/user-attachments/assets/8942c8f4-1366-4c30-b098-71968f939815" />

It's actually standard practice that different activation functions are used on different layers.



A typical modern network might look like:

Hidden layers: ReLU (or variants like Leaky ReLU, GELU)
Output layer: Depends on the task

Sigmoid for binary classification
Softmax for multi-class classification
Linear (no activation) for regression

## Activation functions

Besides the sigmoid function being used in the output layer for binary classification, it's more common to see the **tanh** activation function used in a hidden layer

$$a = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

The tanh function is bound from -1 to 1 on the y axis

<img width="369" height="160" alt="image" src="https://github.com/user-attachments/assets/e3dcc6f4-5697-4c27-bcaf-77d6c9325769" />

Another very common activation function to see in the hidden layer is **RELU** which is $a = max(0,z)$ where z is a positive function 

<img width="473" height="233" alt="image" src="https://github.com/user-attachments/assets/23427109-f4c8-463d-95f8-405ea1168ae6" />

Sometimes people will also use **leaky RELU** where there is a slight bend 

<img width="571" height="309" alt="image" src="https://github.com/user-attachments/assets/ef6fbad2-3da5-4834-aa24-f9e0a9e2d3b1" />
